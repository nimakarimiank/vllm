 vllm serve deepseek-ai/deepseek-llm-7b-chat \
    --tensor-parallel-size 1 \
    --pipeline-parallel-size 2 \
    --distributed-executor-backend ray
INFO 11-28 06:19:52 [__init__.py:235] Automatically detected platform cuda.
INFO 11-28 06:19:55 [api_server.py:1755] vLLM API server version 0.10.1.dev1+gbcc0a3cbe
INFO 11-28 06:19:55 [cli_args.py:261] non-default args: {'model_tag': 'deepseek-ai/deepseek-llm-7b-chat', 'model': 'deepseek-ai/deepseek-llm-7b-chat', 'distributed_executor_backend': 'ray', 'pipeline_parallel_size': 2}
INFO 11-28 06:19:59 [config.py:1604] Using max model len 4096
INFO 11-28 06:19:59 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 11-28 06:20:01 [__init__.py:235] Automatically detected platform cuda.
INFO 11-28 06:20:02 [core.py:572] Waiting for init message from front-end.
INFO 11-28 06:20:02 [core.py:71] Initializing a V1 LLM engine (v0.10.1.dev1+gbcc0a3cbe) with config: model='deepseek-ai/deepseek-llm-7b-chat', speculative_config=None, tokenizer='deepseek-ai/deepseek-llm-7b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/deepseek-llm-7b-chat, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
2025-11-28 06:20:02,753	INFO worker.py:1747 -- Connecting to existing Ray cluster at address: 192.168.0.101:6379...
2025-11-28 06:20:02,760	INFO worker.py:1927 -- Connected to Ray cluster.
INFO 11-28 06:20:03 [ray_utils.py:336] No current placement group found. Creating a new placement group.
INFO 11-28 06:20:03 [ray_distributed_executor.py:169] use_ray_spmd_worker: True
(pid=475) INFO 11-28 06:20:05 [__init__.py:235] Automatically detected platform cuda.
INFO 11-28 06:20:10 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
INFO 11-28 06:20:10 [ray_env.py:65] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USAGE_SOURCE', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_USE_RAY_SPMD_WORKER']
INFO 11-28 06:20:10 [ray_env.py:68] If certain env vars should NOT be copied, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file
ERROR 11-28 06:20:14 [core.py:632] EngineCore failed to start.
ERROR 11-28 06:20:14 [core.py:632] Traceback (most recent call last):
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 623, in run_engine_core
ERROR 11-28 06:20:14 [core.py:632]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 11-28 06:20:14 [core.py:632]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 441, in __init__
ERROR 11-28 06:20:14 [core.py:632]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 77, in __init__
ERROR 11-28 06:20:14 [core.py:632]     self.model_executor = executor_class(vllm_config)
ERROR 11-28 06:20:14 [core.py:632]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 263, in __init__
ERROR 11-28 06:20:14 [core.py:632]     super().__init__(*args, **kwargs)
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 53, in __init__
ERROR 11-28 06:20:14 [core.py:632]     self._init_executor()
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py", line 47, in _init_executor
ERROR 11-28 06:20:14 [core.py:632]     super()._init_executor()
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py", line 107, in _init_executor
ERROR 11-28 06:20:14 [core.py:632]     self._init_workers_ray(placement_group)
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py", line 377, in _init_workers_ray
ERROR 11-28 06:20:14 [core.py:632]     self._run_workers("init_device")
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py", line 503, in _run_workers
ERROR 11-28 06:20:14 [core.py:632]     ray_worker_outputs = ray.get(ray_worker_outputs)
ERROR 11-28 06:20:14 [core.py:632]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
ERROR 11-28 06:20:14 [core.py:632]     return fn(*args, **kwargs)
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
ERROR 11-28 06:20:14 [core.py:632]     return func(*args, **kwargs)
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2858, in get
ERROR 11-28 06:20:14 [core.py:632]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
ERROR 11-28 06:20:14 [core.py:632]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 958, in get_objects
ERROR 11-28 06:20:14 [core.py:632]     raise value.as_instanceof_cause()
ERROR 11-28 06:20:14 [core.py:632] ray.exceptions.RayTaskError(RuntimeError): ray::RayWorkerWrapper.execute_method() (pid=280, ip=192.168.0.103, actor_id=25266910aeac1da8e851133701000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x767036ff6900>)
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 620, in execute_method
ERROR 11-28 06:20:14 [core.py:632]     raise e
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 611, in execute_method
ERROR 11-28 06:20:14 [core.py:632]     return run_method(self, method, args, kwargs)
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 2985, in run_method
ERROR 11-28 06:20:14 [core.py:632]     return func(*args, **kwargs)
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 603, in init_device
ERROR 11-28 06:20:14 [core.py:632]     self.worker.init_device()  # type: ignore
ERROR 11-28 06:20:14 [core.py:632]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 181, in init_device
ERROR 11-28 06:20:14 [core.py:632]     init_worker_distributed_environment(self.vllm_config, self.rank,
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 578, in init_worker_distributed_environment
ERROR 11-28 06:20:14 [core.py:632]     init_distributed_environment(parallel_config.world_size, rank,
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 994, in init_distributed_environment
ERROR 11-28 06:20:14 [core.py:632]     _WORLD = init_world_group(ranks, local_rank, backend)
ERROR 11-28 06:20:14 [core.py:632]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 841, in init_world_group
ERROR 11-28 06:20:14 [core.py:632]     return GroupCoordinator(
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 228, in __init__
ERROR 11-28 06:20:14 [core.py:632]     cpu_group = torch.distributed.new_group(ranks, backend="gloo")
ERROR 11-28 06:20:14 [core.py:632]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py", line 95, in wrapper
ERROR 11-28 06:20:14 [core.py:632]     func_return = func(*args, **kwargs)
ERROR 11-28 06:20:14 [core.py:632]                   ^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5065, in new_group
ERROR 11-28 06:20:14 [core.py:632]     return _new_group_with_tag(
ERROR 11-28 06:20:14 [core.py:632]            ^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5155, in _new_group_with_tag
ERROR 11-28 06:20:14 [core.py:632]     pg, pg_store = _new_process_group_helper(
ERROR 11-28 06:20:14 [core.py:632]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 1942, in _new_process_group_helper
ERROR 11-28 06:20:14 [core.py:632]     backend_class = ProcessGroupGloo(
ERROR 11-28 06:20:14 [core.py:632]                     ^^^^^^^^^^^^^^^^^
ERROR 11-28 06:20:14 [core.py:632] RuntimeError: makeDeviceForInterface(): unsupported gloo device
Process EngineCore_0:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 636, in run_engine_core
    raise e
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 623, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 441, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 77, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 263, in __init__
    super().__init__(*args, **kwargs)
  File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 53, in __init__
    self._init_executor()
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py", line 47, in _init_executor
    super()._init_executor()
  File "/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py", line 107, in _init_executor
    self._init_workers_ray(placement_group)
  File "/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py", line 377, in _init_workers_ray
    self._run_workers("init_device")
  File "/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py", line 503, in _run_workers
    ray_worker_outputs = ray.get(ray_worker_outputs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2858, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 958, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): ray::RayWorkerWrapper.execute_method() (pid=280, ip=192.168.0.103, actor_id=25266910aeac1da8e851133701000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x767036ff6900>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 620, in execute_method
    raise e
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 611, in execute_method
    return run_method(self, method, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 2985, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 603, in init_device
    self.worker.init_device()  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 181, in init_device
    init_worker_distributed_environment(self.vllm_config, self.rank,
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 578, in init_worker_distributed_environment
    init_distributed_environment(parallel_config.world_size, rank,
  File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 994, in init_distributed_environment
    _WORLD = init_world_group(ranks, local_rank, backend)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 841, in init_world_group
    return GroupCoordinator(
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 228, in __init__
    cpu_group = torch.distributed.new_group(ranks, backend="gloo")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py", line 95, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5065, in new_group
    return _new_group_with_tag(
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5155, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 1942, in _new_process_group_helper
    backend_class = ProcessGroupGloo(
                    ^^^^^^^^^^^^^^^^^
RuntimeError: makeDeviceForInterface(): unsupported gloo device
INFO 11-28 06:20:14 [ray_distributed_executor.py:120] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.
2025-11-28 06:20:14,230	ERROR worker.py:427 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::RayWorkerWrapper.execute_method() (pid=475, ip=192.168.0.101, actor_id=b4e42d42231a41a48213e0fe01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7f91bbd45c40>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 620, in execute_method
    raise e
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 611, in execute_method
    return run_method(self, method, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 2985, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 603, in init_device
    self.worker.init_device()  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 181, in init_device
    init_worker_distributed_environment(self.vllm_config, self.rank,
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 578, in init_worker_distributed_environment
    init_distributed_environment(parallel_config.world_size, rank,
  File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 994, in init_distributed_environment
    _WORLD = init_world_group(ranks, local_rank, backend)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 841, in init_world_group
    return GroupCoordinator(
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 228, in __init__
    cpu_group = torch.distributed.new_group(ranks, backend="gloo")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py", line 95, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5065, in new_group
    return _new_group_with_tag(
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5155, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 1942, in _new_process_group_helper
    backend_class = ProcessGroupGloo(
                    ^^^^^^^^^^^^^^^^^
RuntimeError: makeDeviceForInterface(): unsupported gloo device
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619] Error executing method 'init_device'. This might cause deadlock in distributed execution.
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619] Traceback (most recent call last):
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 611, in execute_method
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     return run_method(self, method, args, kwargs)
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 2985, in run_method
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     return func(*args, **kwargs)
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/ray/util/tracing/tracing_helper.py", line 461, in _resume_span
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     return method(self, *_args, **_kwargs)
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 603, in init_device
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     self.worker.init_device()  # type: ignore
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 181, in init_device
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     init_worker_distributed_environment(self.vllm_config, self.rank,
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 578, in init_worker_distributed_environment
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     init_distributed_environment(parallel_config.world_size, rank,
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 994, in init_distributed_environment
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     _WORLD = init_world_group(ranks, local_rank, backend)
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 841, in init_world_group
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     return GroupCoordinator(
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 228, in __init__
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     cpu_group = torch.distributed.new_group(ranks, backend="gloo")
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py", line 95, in wrapper
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     func_return = func(*args, **kwargs)
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]                   ^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5065, in new_group
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     return _new_group_with_tag(
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5155, in _new_group_with_tag
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     pg, pg_store = _new_process_group_helper(
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 1942, in _new_process_group_helper
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]     backend_class = ProcessGroupGloo(
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619]                     ^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=475) ERROR 11-28 06:20:14 [worker_base.py:619] RuntimeError: makeDeviceForInterface(): unsupported gloo device
(pid=280, ip=192.168.0.103) INFO 11-28 06:20:09 [__init__.py:235] Automatically detected platform cuda.
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619] Error executing method 'init_device'. This might cause deadlock in distributed execution.
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619] Traceback (most recent call last):
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 611, in execute_method
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     return run_method(self, method, args, kwargs)
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 2985, in run_method
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     return func(*args, **kwargs)
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]                   ^^^^^^^^^^^^^^^^^^^^^ [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/ray/util/tracing/tracing_helper.py", line 461, in _resume_span
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     return method(self, *_args, **_kwargs)
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 181, in init_device [repeated 2x across cluster]
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     self.worker.init_device()  # type: ignore
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     init_worker_distributed_environment(self.vllm_config, self.rank,
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 578, in init_worker_distributed_environment
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     init_distributed_environment(parallel_config.world_size, rank,
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 994, in init_distributed_environment
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     _WORLD = init_world_group(ranks, local_rank, backend)
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 841, in init_world_group
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     return GroupCoordinator(
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]                     ^^^^^^^^^^^^^^^^^ [repeated 2x across cluster]
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py", line 228, in __init__
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     cpu_group = torch.distributed.new_group(ranks, backend="gloo")
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py", line 95, in wrapper
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     func_return = func(*args, **kwargs)
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5065, in new_group
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     return _new_group_with_tag(
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]            ^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 5155, in _new_group_with_tag
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     pg, pg_store = _new_process_group_helper(
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 1942, in _new_process_group_helper
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619]     backend_class = ProcessGroupGloo(
(RayWorkerWrapper pid=280, ip=192.168.0.103) ERROR 11-28 06:20:14 [worker_base.py:619] RuntimeError: makeDeviceForInterface(): unsupported gloo device
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py", line 54, in main
    args.dispatch_function(args)
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py", line 52, in cmd
    uvloop.run(run_server(args))
  File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1791, in run_server
    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1811, in run_server_worker
    async with build_async_engine_client(args, client_config) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 158, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 194, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 163, in from_vllm_config
    return cls(
           ^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 117, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 98, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 677, in __init__
    super().__init__(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 408, in __init__
    with launch_core_engines(vllm_config, executor_class,
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 697, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 750, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
root@dsosws01:/vllm-workspace# ray status
======== Autoscaler status: 2025-11-28 06:20:43.452981 ========
Node status
---------------------------------------------------------------
Active:
 1 node_283f46d24d5761593437d0eb015c9f963a6872008e9503d7bfce5f1a
 1 node_4e8a56259566a8bf1bbb59d4277037eedb2d84cdad6f653a854e7e4a
 1 node_701d78d446840eb73d98360110a21562f32fae4ae68a7ee82630fcec
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Total Usage:
 0.0/48.0 CPU
 0.0/3.0 GPU
 0B/65.07GiB memory
 0B/27.89GiB object_store_memory

Total Constraints:
 (no request_resources() constraints)
Total Demands:
 (no resource demands)
root@dsosws01:/vllm-workspace# 

